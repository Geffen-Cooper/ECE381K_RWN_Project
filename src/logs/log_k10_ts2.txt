1669772795.640350491
=================
Namespace(k=10, dataset='arxiv')
training configuration:
Number of partitions: 10
Dataset: arxiv
LOAD ARXIV
cuda
DglNodePropPredDataset(1)
------
 (Graph(num_nodes=169343, num_edges=1166243,
      ndata_schemes={'year': Scheme(shape=(1,), dtype=torch.int64), 'feat': Scheme(shape=(128,), dtype=torch.float32)}
      edata_schemes={}), tensor([[ 4],
        [ 5],
        [28],
        ...,
        [10],
        [ 4],
        [ 1]]))
Number of categories: 40
partitioning  10  subgraphs
   RES
118996
1669772849.351556951
   RES
200180
1669772849.505907023
   RES
231744
1669772849.660362389
   RES
256984
1669772849.814433499
   RES
275580
1669772849.968782811
   RES
283648
1669772850.123708486
   RES
293632
1669772850.278490996
   RES
329208
1669772850.434123975
   RES
389340
1669772850.591060015
   RES
428848
1669772850.749738030
   RES
468580
1669772850.905260544
   RES
477776
1669772851.064080008
   RES
518176
1669772851.221786036
   RES
534544
1669772851.382934343
   RES
534544
1669772851.542487380
   RES
525232
1669772851.702201562
   RES
517540
1669772851.861758625
   RES
517804
1669772852.027138297
   RES
518224
1669772852.191094080
   RES
517964
1669772852.349974907
   RES
517964
1669772852.510779122
   RES
517964
1669772852.673110574
   RES
520072
1669772852.835755291
   RES
516224
1669772853.010507931
   RES
516664
1669772853.196441800
   RES
493136
1669772853.357457781
   RES
485348
1669772853.524689655
   RES
485092
1669772853.689671886
   RES
517824
1669772853.854362047
   RES
517824
1669772854.016690364
   RES
518224
1669772854.180897266
   RES
517968
1669772854.342184776
   RES
492924
1669772854.504862987
   RES
501368
1669772854.665409892
   RES
501016
1669772854.829252167
   RES
498576
1669772854.992328808
   RES
498836
1669772855.155987593
   RES
498580
1669772855.318912088
   RES
498580
1669772855.481093542
   RES
498580
1669772855.645513857
   RES
498832
1669772855.810621344
   RES
498316
1669772855.978847356
   RES
504776
1669772856.148012456
   RES
495736
1669772856.309744914
   RES
504712
1669772856.474123759
   RES
504644
1669772856.637064024
   RES
504124
1669772856.799141851
   RES
512096
1669772856.965103233
   RES
512700
1669772857.124979962
   RES
512440
1669772857.286940226
   RES
521148
1669772857.450326750
   RES
520660
1669772857.612407583
   RES
520920
1669772857.772739768
   RES
520920
1669772857.933673201
   RES
520664
1669772858.096874021
   RES
518464
1669772858.267502865
   RES
494076
1669772858.434225151
   RES
485728
1669772858.598544821
   RES
527172
1669772858.760222169
   RES
526960
1669772858.922056167
   RES
501656
1669772859.085971538
   RES
501908
1669772859.248518774
   RES
501908
1669772859.410020080
   RES
501652
1669772859.572199384
   RES
526464
1669772859.737087657
   RES
526448
1669772859.899158583
   RES
510136
1669772860.061046572
   RES
510132
1669772860.221735143
   RES
510132
1669772860.381009051
   RES
510136
1669772860.538842642
   RES
509880
1669772860.698963257
   RES
510140
1669772860.861229702
   RES
509876
1669772861.019963089
   RES
515680
1669772861.178119921
   RES
515404
1669772861.336848567
   RES
532296
1669772861.502044236
   RES
532096
1669772861.661281109
   RES
531840
1669772861.821586737
   RES
532100
1669772861.980163848
   RES
507312
1669772862.138766060
   RES
523940
1669772862.298766901
   RES
523620
1669772862.459921709
   RES
515488
1669772862.622786003
   RES
515752
1669772862.783647473
   RES
515144
1669772862.947690345
   RES
532300
1669772863.111522482
   RES
531580
1669772863.277852158
   RES
523184
1669772863.441023589
   RES
514964
1669772863.604673569
   RES
515140
1669772863.764630684
   RES
523584
1669772863.923440071
   RES
524112
1669772864.082256720
   RES
523368
1669772864.240175207
   RES
532340
1669772864.400744650
   RES
531580
1669772864.560535775
   RES
531580
1669772864.721109249
   RES
507048
1669772864.882396428
   RES
507572
1669772865.043424602
   RES
507056
1669772865.204080594
   RES
507056
1669772865.366136006
   RES
507580
1669772865.524326826
   RES
507052
1669772865.682506288
   RES
515760
1669772865.843247492
   RES
515148
1669772866.001750450
   RES
523880
1669772866.161833244
   RES
531544
1669772866.323560550
   RES
515272
1669772866.483180435
   RES
515272
1669772866.648246162
   RES
515528
1669772866.808494238
   RES
539556
1669772866.970194878
   RES
540324
1669772867.134407004
   RES
539808
1669772867.293937359
   RES
515276
1669772867.458670065
   RES
523192
1669772867.622622243
   RES
523748
1669772867.785546388
   RES
524008
1669772867.948538815
   RES
539592
1669772868.107812927
   RES
540324
1669772868.266892381
   RES
540064
1669772868.429719448
   RES
515532
1669772868.594183490
   RES
515788
1669772868.754312154
   RES
515532
1669772868.911810402
   RES
512316
1669772869.072791891
   RES
512060
1669772869.231442733
   RES
528160
1669772869.391476712
   RES
536344
1669772869.550243796
   RES
536592
1669772869.709456819
   RES
515016
1669772869.869832875
   RES
515276
1669772870.033960437
   RES
515016
1669772870.193981632
   RES
515276
1669772870.353592568
   RES
515276
1669772870.511665050
   RES
515020
1669772870.671154271
   RES
524112
1669772870.831629204
   RES
524112
1669772870.991665398
   RES
524112
1669772871.152812285
   RES
524116
1669772871.312920507
   RES
524112
1669772871.474207858
   RES
524112
1669772871.633491198
   RES
523852
1669772871.793906547
   RES
524112
1669772871.955843950
   RES
523856
1669772872.115335324
   RES
524108
1669772872.273666526
   RES
523852
1669772872.434547149
   RES
523852
1669772872.597655196
   RES
524112
1669772872.758180092
   RES
524112
1669772872.917017780
   RES
523856
1669772873.075247049
   RES
518872
1669772873.234010991
   RES
518616
1669772873.395524694
   RES
518868
1669772873.556365060
   RES
502172
1669772873.713522858
   RES
534904
1669772873.875294116
   RES
526832
1669772874.037750996
   RES
502172
1669772874.200810647
   RES
518272
1669772874.361519395
   RES
499088
1669772874.519415286
   RES
507308
1669772874.679757911
   RES
507568
1669772874.840864471
   RES
507572
1669772875.001189029
   RES
507316
1669772875.159171590
   RES
512968
1669772875.318574347
   RES
509460
1669772875.478651667
   RES
503252
1669772875.640160232
   RES
511900
1669772875.799294856
   RES
532272
1669772875.956867303
   RES
528256
1669772876.117087553
   RES
528000
1669772876.278475628
   RES
528000
1669772876.436672904
   RES
528004
1669772876.597378153
   RES
518616
1669772876.759759006
   RES
492484
1669772876.921194270
   RES
489800
1669772877.081776928
   RES
497460
1669772877.241955724
   RES
497888
1669772877.401165972
   RES
519540
1669772877.563721845
   RES
495192
1669772877.723860988
   RES
495452
1669772877.885014515
   RES
511260
1669772878.050459030
   RES
511760
1669772878.214546832
   RES
511504
1669772878.371255373
   RES
511764
1669772878.532550175
   RES
511508
1669772878.696471088
   RES
511508
1669772878.858781496
   RES
511756
1669772879.017293970
   RES
477972
1669772879.176776346
   RES
460812
1669772879.336775531
   RES
468472
1669772879.496097063
   RES
468900
1669772879.659791086
   RES
468644
1669772879.818567014
   RES
476688
1669772879.978554964
   RES
476992
1669772880.141039608
   RES
476992
1669772880.304224371
   RES
484912
1669772880.495203919
   RES
455680
1669772880.655309569
   RES
463340
1669772880.813057738
   RES
475292
1669772880.971683524
   RES
512964
1669772881.136415578
   RES
513224
1669772881.297798970
   RES
513216
1669772881.456105567
   RES
512960
1669772881.616649595
   RES
487924
1669772881.776061734
   RES
535336
1669772881.936042273
   RES
513328
1669772882.097500097
   RES
540896
1669772882.256474723
   RES
563104
1669772882.417451698
   RES
562772
1669772882.577814199
   RES
546460
1669772882.741113667
   RES
496836
1669772882.903995187
   RES
546204
1669772883.069243170
   RES
554680
1669772883.232896809
   RES
537936
1669772883.396143819
   RES
533460
1669772883.560855739
   RES
543376
1669772883.724695659
   RES
559476
1669772883.888682249
   RES
567660
1669772884.050207478
   RES
542560
1669772884.211707784
   RES
542560
1669772884.374356050
   RES
504540
1669772884.536334291
   RES
512724
1669772884.697631081
   RES
521512
1669772884.859909105
   RES
521452
1669772885.024431294
   RES
521160
1669772885.187319494
   RES
570352
1669772885.348478270
   RES
570880
1669772885.509386558
   RES
570476
1669772885.670355365
   RES
554164
1669772885.836016988
   RES
554424
1669772886.000868716
   RES
502868
1669772886.162130966
   RES
543968
1669772886.323422753
   RES
546076
1669772886.488386477
   RES
576700
1669772886.650355833
   RES
576700
1669772886.811527742
   RES
496756
1669772886.973592527
   RES
530812
1669772887.139017827
   RES
511088
1669772887.300475673
   RES
552004
1669772887.462195470
   RES
552004
1669772887.623286820
   RES
511088
1669772887.784690935
   RES
542856
1669772887.952024969
   RES
534624
1669772888.119126511
   RES
542808
1669772888.283719837
   RES
534928
1669772888.448397973
   RES
551064
1669772888.609621321
   RES
562644
1669772888.771862723
   RES
575844
1669772888.934514094
   RES
575844
1669772889.098346382
   RES
542844
1669772889.260513188
   RES
557364
1669772889.423317345
   RES
543148
1669772889.586603049
   RES
543148
1669772889.750734095
   RES
543148
1669772889.912095469
   RES
543148
1669772890.074737064
   RES
543148
1669772890.236260282
   RES
543148
1669772890.401322999
   RES
543148
1669772890.566861312
   RES
501852
1669772890.732781116
   RES
546460
1669772890.896836126
   RES
546460
1669772891.060402126
   RES
516148
1669772891.222779840
   RES
534888
1669772891.383922533
   RES
516000
1669772891.541983200
   RES
532284
1669772891.711621009
   RES
540468
1669772891.873141091
   RES
543900
1669772892.033789657
   RES
557100
1669772892.196033750
   RES
565284
1669772892.361308843
   RES
565684
1669772892.525601164
   RES
565684
1669772892.685760193
   RES
565348
1669772892.847223026
   RES
549036
1669772893.008624305
   RES
557384
1669772893.168947039
   RES
557384
1669772893.330356827
   RES
534960
1669772893.498517552
   RES
513844
1669772893.660909545
   RES
513844
1669772893.821104828
   RES
522064
1669772893.983803516
   RES
522064
1669772894.149582732
   RES
522064
1669772894.312217756
   RES
522064
1669772894.472662508
   RES
522068
1669772894.634199373
   RES
522068
1669772894.794048894
   RES
554948
1669772894.954958533
   RES
554948
1669772895.116977805
   RES
579424
1669772895.277791468
   RES
587872
1669772895.439508022
   RES
587700
1669772895.597949460
   RES
587700
1669772895.760218157
   RES
571388
1669772895.922640911
   RES
571388
1669772896.082589969
   RES
513804
1669772896.242689870
   RES
544548
1669772896.404860004
   RES
536460
1669772896.564838435
   RES
528368
1669772896.728884289
   RES
577468
1669772896.892469668
   RES
577732
1669772897.051595671
   RES
577560
1669772897.210849384
   RES
561248
1669772897.370453520
   RES
561248
1669772897.531845166
   RES
483300
1669772897.690915991
   RES
499740
1669772897.849493473
   RES
507920
1669772898.007704888
   RES
532436
1669772898.168846894
   RES
540800
1669772898.331642504
   RES
524400
1669772898.491969413
   RES
565316
1669772898.655267139
   RES
565316
1669772898.817049128
   RES
565580
1669772898.978171847
   RES
565120
1669772899.148647731
   RES
565120
1669772899.317473890
   RES
565376
1669772899.477502734
   RES
551560
1669772899.636782361
   RES
557368
1669772899.795649567
   RES
556900
1669772899.955126738
   RES
534632
1669772900.120110679
   RES
534632
1669772900.279639242
   RES
534808
1669772900.440117226
   RES
534808
1669772900.606679063
   RES
518600
1669772900.769499195
   RES
562688
1669772900.927389167
   RES
563036
1669772901.087454951
   RES
562780
1669772901.249542852
   RES
563040
1669772901.407416838
   RES
563036
1669772901.567190572
   RES
562780
1669772901.728000272
   RES
538248
1669772901.888191040
   RES
538504
1669772902.050089980
   RES
526568
1669772902.211735575
   RES
535388
1669772902.371222986
   RES
535036
1669772902.531875389
   RES
567768
1669772902.698095982
   RES
567768
1669772902.860154494
   RES
568296
1669772903.019867272
   RES
568180
1669772903.179892825
   RES
568180
1669772903.339329961
   RES
567924
1669772903.499304656
   RES
568184
1669772903.659644459
   RES
568180
1669772903.817536495
   RES
532552
1669772903.977371749
   RES
532728
1669772904.135943651
   RES
565552
1669772904.298219603
   RES
565608
1669772904.460247996
   RES
565608
1669772904.620652679
   RES
565600
1669772904.780837630
   RES
532104
1669772904.940136958
   RES
532104
1669772905.101288114
   RES
548204
1669772905.261023163
   RES
556764
1669772905.421854052
   RES
556764
1669772905.582046960
   RES
556764
1669772905.744641077
   RES
556764
1669772905.909986098
   RES
556764
1669772906.073023501
   RES
532108
1669772906.233306567
   RES
528068
1669772906.395846728
   RES
528244
1669772906.558538153
   RES
566520
1669772906.719624505
   RES
569424
1669772906.884997814
   RES
553032
1669772907.050403012
   RES
561212
1669772907.210842097
   RES
561124
1669772907.368210354
   RES
561124
1669772907.529490442
   RES
535976
1669772907.689724773
   RES
543892
1669772907.849390215
   RES
542592
1669772908.015966909
   RES
542592
1669772908.179966128
   RES
525760
1669772908.340231803
   RES
525672
1669772908.499406188
   RES
524292
1669772908.660746281
   RES
524472
1669772908.820267918
   RES
535456
1669772908.979586810
   RES
540736
1669772909.138821134
   RES
540912
1669772909.296577569
   RES
524344
1669772909.456271516
   RES
532524
1669772909.618701049
   RES
515928
1669772909.777829750
   RES
540284
1669772909.939581377
   RES
540716
1669772910.099753986
   RES
548432
1669772910.257936480
   RES
556880
1669772910.415596783
   RES
557144
1669772910.574510340
   RES
540580
1669772910.733716754
   RES
539568
1669772910.894285144
=================
Namespace(gnn='GCN', k=10, i=6, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.796, train acc: 0.004, val loss: 3.791, val acc: 0.010 (best val acc: 0.010))
In epoch 20, train loss: 1.002, train acc: 0.771, val loss: 1.153, val acc: 0.715 (best val acc: 0.715))
In epoch 40, train loss: 0.811, train acc: 0.783, val loss: 0.984, val acc: 0.719 (best val acc: 0.722))
In epoch 60, train loss: 0.727, train acc: 0.791, val loss: 0.905, val acc: 0.728 (best val acc: 0.728))
In epoch 80, train loss: 0.680, train acc: 0.800, val loss: 0.861, val acc: 0.728 (best val acc: 0.729))
In epoch 0, student train loss: 3.241, student train acc: 0.008, student val loss: 3.262, student val acc: 0.009 (best student val acc: 0.009))
In epoch 20, student train loss: 1.065, student train acc: 0.719, student val loss: 1.256, student val acc: 0.622 (best student val acc: 0.622))
In epoch 40, student train loss: 0.967, student train acc: 0.719, student val loss: 1.122, student val acc: 0.622 (best student val acc: 0.622))
In epoch 60, student train loss: 0.917, student train acc: 0.719, student val loss: 1.074, student val acc: 0.622 (best student val acc: 0.622))
In epoch 80, student train loss: 0.865, student train acc: 0.722, student val loss: 1.015, student val acc: 0.628 (best student val acc: 0.628))
   RES
539744
1669772911.052682773
   RES
539744
1669772911.212040330
   RES
540000
1669772911.370172744
   RES
539744
1669772911.529859375
   RES
529512
1669772911.689153859
   RES
529776
1669772911.847531978
   RES
553540
1669772912.007971466
   RES
554332
1669772912.166801970
   RES
554052
1669772912.327137456
   RES
537700
1669772912.489420005
   RES
537956
1669772912.654354777
   RES
537700
1669772912.817342340
   RES
545616
1669772912.980337196
   RES
546144
1669772913.140136981
   RES
546044
1669772913.298373776
   RES
545788
1669772913.458838893
   RES
546048
1669772913.619137854
   RES
545792
1669772913.776650366
   RES
545616
1669772913.947603448
   RES
545872
1669772914.107899143
   RES
545872
1669772914.267843857
   RES
554320
1669772914.428385580
   RES
554848
1669772914.591012898
   RES
554564
1669772914.752598715
   RES
547716
1669772914.915021001
   RES
538476
1669772915.074731473
   RES
530032
1669772915.236266167
   RES
546132
1669772915.397772294
   RES
546604
1669772915.561749277
   RES
546604
1669772915.728341178
   RES
546348
1669772915.888232850
   RES
530260
1669772916.050728052
   RES
513852
1669772916.213920215
=================
Namespace(gnn='GCN', k=10, i=5, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.633, train acc: 0.005, val loss: 3.631, val acc: 0.005 (best val acc: 0.005))
In epoch 20, train loss: 1.531, train acc: 0.634, val loss: 1.290, val acc: 0.663 (best val acc: 0.668))
In epoch 40, train loss: 1.270, train acc: 0.692, val loss: 1.137, val acc: 0.703 (best val acc: 0.703))
In epoch 60, train loss: 1.105, train acc: 0.721, val loss: 1.026, val acc: 0.720 (best val acc: 0.723))
In epoch 80, train loss: 1.011, train acc: 0.737, val loss: 0.950, val acc: 0.742 (best val acc: 0.742))
In epoch 0, student train loss: 3.327, student train acc: 0.006, student val loss: 3.328, student val acc: 0.002 (best student val acc: 0.002))
In epoch 20, student train loss: 1.555, student train acc: 0.630, student val loss: 1.290, student val acc: 0.661 (best student val acc: 0.661))
In epoch 40, student train loss: 1.454, student train acc: 0.630, student val loss: 1.230, student val acc: 0.661 (best student val acc: 0.662))
In epoch 60, student train loss: 1.382, student train acc: 0.631, student val loss: 1.178, student val acc: 0.661 (best student val acc: 0.662))
In epoch 80, student train loss: 1.306, student train acc: 0.634, student val loss: 1.132, student val acc: 0.662 (best student val acc: 0.662))
   RES
562952
1669772916.374181188
   RES
563216
1669772916.536109249
   RES
563044
1669772916.707318231
   RES
571224
1669772916.871172596
   RES
571264
1669772917.034122141
   RES
571264
1669772917.196928995
   RES
571264
1669772917.358765369
   RES
545508
1669772917.521099981
   RES
545772
1669772917.680115731
   RES
545472
1669772917.844036125
   RES
553920
1669772918.004936846
   RES
553904
1669772918.164456189
   RES
559828
1669772918.331702340
=================
Namespace(gnn='GCN', k=10, i=7, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.707, train acc: 0.075, val loss: 3.677, val acc: 0.112 (best val acc: 0.112))
In epoch 20, train loss: 1.640, train acc: 0.646, val loss: 1.937, val acc: 0.550 (best val acc: 0.550))
In epoch 40, train loss: 1.304, train acc: 0.688, val loss: 1.569, val acc: 0.579 (best val acc: 0.579))
In epoch 60, train loss: 1.118, train acc: 0.715, val loss: 1.373, val acc: 0.636 (best val acc: 0.636))
In epoch 80, train loss: 1.013, train acc: 0.733, val loss: 1.255, val acc: 0.672 (best val acc: 0.672))
In epoch 0, student train loss: 3.338, student train acc: 0.020, student val loss: 3.325, student val acc: 0.049 (best student val acc: 0.049))
In epoch 20, student train loss: 1.972, student train acc: 0.495, student val loss: 2.257, student val acc: 0.321 (best student val acc: 0.322))
In epoch 40, student train loss: 1.757, student train acc: 0.504, student val loss: 2.006, student val acc: 0.358 (best student val acc: 0.358))
In epoch 60, student train loss: 1.586, student train acc: 0.608, student val loss: 1.834, student val acc: 0.503 (best student val acc: 0.503))
In epoch 80, student train loss: 1.450, student train acc: 0.635, student val loss: 1.705, student val acc: 0.516 (best student val acc: 0.519))
   RES
536600
1669772918.487889454
   RES
536600
1669772918.644743707
   RES
569332
1669772918.801721653
   RES
569332
1669772918.957771145
   RES
529988
1669772919.114826708
=================
Namespace(gnn='GCN', k=10, i=4, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.629, train acc: 0.002, val loss: 3.612, val acc: 0.003 (best val acc: 0.003))
In epoch 20, train loss: 1.610, train acc: 0.649, val loss: 1.226, val acc: 0.742 (best val acc: 0.742))
In epoch 40, train loss: 1.204, train acc: 0.709, val loss: 0.934, val acc: 0.780 (best val acc: 0.780))
In epoch 60, train loss: 1.007, train acc: 0.738, val loss: 0.822, val acc: 0.791 (best val acc: 0.792))
In epoch 80, train loss: 0.900, train acc: 0.761, val loss: 0.776, val acc: 0.801 (best val acc: 0.801))
In epoch 0, student train loss: 3.320, student train acc: 0.011, student val loss: 3.320, student val acc: 0.003 (best student val acc: 0.003))
In epoch 20, student train loss: 1.731, student train acc: 0.633, student val loss: 1.196, student val acc: 0.737 (best student val acc: 0.737))
In epoch 40, student train loss: 1.521, student train acc: 0.640, student val loss: 1.172, student val acc: 0.737 (best student val acc: 0.737))
In epoch 60, student train loss: 1.355, student train acc: 0.661, student val loss: 1.040, student val acc: 0.744 (best student val acc: 0.744))
In epoch 80, student train loss: 1.233, student train acc: 0.675, student val loss: 0.942, student val acc: 0.759 (best student val acc: 0.759))
   RES
571436
1669772919.272190008
   RES
571264
1669772919.427473791
   RES
532860
1669772919.583907150
   RES
521768
1669772919.740761549
   RES
571400
1669772919.895888031
   RES
554964
1669772920.052064020
   RES
563144
1669772920.208071045
   RES
563048
1669772920.363539151
   RES
537900
1669772920.519133373
=================
Namespace(gnn='GCN', k=10, i=3, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.673, train acc: 0.033, val loss: 3.665, val acc: 0.019 (best val acc: 0.019))
In epoch 20, train loss: 2.344, train acc: 0.407, val loss: 2.228, val acc: 0.430 (best val acc: 0.430))
In epoch 40, train loss: 1.662, train acc: 0.542, val loss: 1.711, val acc: 0.512 (best val acc: 0.512))
In epoch 60, train loss: 1.436, train acc: 0.587, val loss: 1.549, val acc: 0.535 (best val acc: 0.535))
In epoch 80, train loss: 1.322, train acc: 0.613, val loss: 1.485, val acc: 0.546 (best val acc: 0.546))
In epoch 0, student train loss: 3.344, student train acc: 0.012, student val loss: 3.341, student val acc: 0.009 (best student val acc: 0.009))
In epoch 20, student train loss: 2.697, student train acc: 0.206, student val loss: 2.425, student val acc: 0.338 (best student val acc: 0.352))
In epoch 40, student train loss: 2.456, student train acc: 0.255, student val loss: 2.237, student val acc: 0.362 (best student val acc: 0.362))
In epoch 60, student train loss: 2.192, student train acc: 0.337, student val loss: 2.004, student val acc: 0.400 (best student val acc: 0.400))
In epoch 80, student train loss: 1.975, student train acc: 0.396, student val loss: 1.843, student val acc: 0.427 (best student val acc: 0.427))
   RES
455612
1669772920.674593141
   RES
455612
1669772920.830364744
   RES
455356
1669772920.986953829
   RES
464660
1669772921.142408239
