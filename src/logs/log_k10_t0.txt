1669772214.494749365
=================
Namespace(k=10, dataset='arxiv')
training configuration:
Number of partitions: 10
Dataset: arxiv
LOAD ARXIV
cuda
DglNodePropPredDataset(1)
------
 (Graph(num_nodes=169343, num_edges=1166243,
      ndata_schemes={'year': Scheme(shape=(1,), dtype=torch.int64), 'feat': Scheme(shape=(128,), dtype=torch.float32)}
      edata_schemes={}), tensor([[ 4],
        [ 5],
        [28],
        ...,
        [10],
        [ 4],
        [ 1]]))
Number of categories: 40
partitioning  10  subgraphs
   RES
 88592
1669772267.228715549
   RES
132508
1669772267.383162235
   RES
197920
1669772267.537601604
   RES
212028
1669772267.692025840
   RES
231512
1669772267.845913230
   RES
254844
1669772268.000324400
   RES
271116
1669772268.154290782
   RES
276488
1669772268.309612268
   RES
280268
1669772268.466062165
   RES
282504
1669772268.622328803
   RES
283124
1669772268.777549007
   RES
293956
1669772268.934729234
   RES
308612
1669772269.092035626
   RES
322476
1669772269.253561404
   RES
329688
1669772269.415083722
   RES
329688
1669772269.575701249
   RES
337828
1669772269.735108892
   RES
410004
1669772269.896186585
   RES
443572
1669772270.059240831
   RES
428592
1669772270.222575251
   RES
437724
1669772270.380204256
   RES
464644
1669772270.543563256
   RES
431880
1669772270.705825408
   RES
431384
1669772270.865942195
   RES
455668
1669772271.028039007
   RES
456044
1669772271.192152152
   RES
456044
1669772271.358018986
   RES
456044
1669772271.521311758
   RES
456304
1669772271.684042275
   RES
456048
1669772271.851694008
   RES
488552
1669772272.020152172
   RES
497000
1669772272.184979415
   RES
497528
1669772272.358787570
   RES
497276
1669772272.536429234
   RES
497020
1669772272.698151382
   RES
496896
1669772272.860064365
   RES
496892
1669772273.021022541
   RES
496636
1669772273.179664923
   RES
504552
1669772273.340140715
   RES
505080
1669772273.502755316
   RES
504984
1669772273.666465553
   RES
504728
1669772273.828041608
   RES
477624
1669772273.991088501
   RES
466828
1669772274.153211123
   RES
491492
1669772274.313867428
   RES
466524
1669772274.477810413
   RES
475236
1669772274.646458348
   RES
475180
1669772274.810059967
   RES
474708
1669772274.972502859
   RES
475052
1669772275.135118229
   RES
475312
1669772275.298944072
   RES
475056
1669772275.460149213
   RES
515972
1669772275.619055988
   RES
515972
1669772275.780198643
   RES
516500
1669772275.944515280
   RES
516024
1669772276.104374635
   RES
524204
1669772276.264746796
   RES
524500
1669772276.423968869
   RES
524244
1669772276.590702972
   RES
473392
1669772276.756362856
   RES
473648
1669772276.917745696
   RES
473648
1669772277.081669418
   RES
473648
1669772277.246138913
   RES
473392
1669772277.405630026
   RES
473392
1669772277.571266548
   RES
479304
1669772277.730558175
   RES
479300
1669772277.888770452
   RES
495400
1669772278.048530122
   RES
479300
1669772278.209119896
   RES
479300
1669772278.368341459
   RES
460288
1669772278.527397057
   RES
460288
1669772278.702257396
   RES
460288
1669772278.913625520
   RES
460288
1669772279.076171709
   RES
451280
1669772279.244208047
   RES
451280
1669772279.401591941
   RES
465004
1669772279.562175895
   RES
451536
1669772279.723944294
   RES
451536
1669772279.882677574
   RES
472976
1669772280.047207114
   RES
475992
1669772280.207478068
   RES
475992
1669772280.366610157
   RES
475992
1669772280.524963786
   RES
483600
1669772280.685171318
   RES
508416
1669772280.844441381
   RES
508696
1669772281.004847030
   RES
508696
1669772281.164864240
   RES
508696
1669772281.324691955
   RES
508696
1669772281.484019285
   RES
508696
1669772281.646877458
   RES
508696
1669772281.803564547
   RES
508700
1669772281.961459099
   RES
508700
1669772282.121653801
   RES
508700
1669772282.289145294
   RES
508700
1669772282.449616999
   RES
508700
1669772282.611973997
   RES
508700
1669772282.770658186
   RES
508700
1669772282.928777455
   RES
508700
1669772283.092492357
   RES
508700
1669772283.254569922
   RES
467212
1669772283.412406284
   RES
467212
1669772283.570854189
   RES
467212
1669772283.730085288
   RES
458736
1669772283.890728276
   RES
500180
1669772284.049335883
   RES
499968
1669772284.206386119
   RES
499964
1669772284.369784702
   RES
524776
1669772284.533801269
   RES
524776
1669772284.691827313
   RES
524632
1669772284.852501012
   RES
524632
1669772285.013799664
   RES
524632
1669772285.177896549
   RES
524632
1669772285.335571379
   RES
524632
1669772285.494596134
   RES
494444
1669772285.655798184
   RES
502668
1669772285.814826526
   RES
502668
1669772285.972959255
   RES
506320
1669772286.133231492
   RES
516616
1669772286.293295147
   RES
516412
1669772286.452952304
   RES
516412
1669772286.612521967
   RES
516412
1669772286.774121358
   RES
516412
1669772286.935327969
   RES
516412
1669772287.096178280
   RES
516412
1669772287.257650351
   RES
516412
1669772287.420018736
   RES
516412
1669772287.581363474
   RES
516412
1669772287.741846645
   RES
508012
1669772287.901850006
   RES
500096
1669772288.061222636
   RES
500096
1669772288.220302265
   RES
500096
1669772288.380773592
   RES
508012
1669772288.542713368
   RES
508012
1669772288.703882905
   RES
507928
1669772288.862312147
   RES
507928
1669772289.022037283
   RES
507928
1669772289.180939859
   RES
508188
1669772289.341358269
   RES
508184
1669772289.499441308
   RES
507928
1669772289.661757896
   RES
508188
1669772289.820364653
   RES
507928
1669772289.979963408
   RES
475668
1669772290.140214636
   RES
459224
1669772290.300900900
   RES
459224
1669772290.459260211
   RES
459480
1669772290.616561868
   RES
459224
1669772290.776360065
   RES
467140
1669772290.936873318
   RES
467668
1669772291.095280709
   RES
467316
1669772291.253361547
   RES
467104
1669772291.412600938
   RES
467444
1669772291.574122499
   RES
467700
1669772291.735035659
   RES
467700
1669772291.894449394
   RES
451004
1669772292.055526658
   RES
485848
1669772292.221545167
   RES
475928
1669772292.384012509
   RES
451260
1669772292.543120834
   RES
492176
1669772292.706441147
   RES
500624
1669772292.867032470
   RES
500624
1669772293.023518384
   RES
516820
1669772293.184108322
   RES
525004
1669772293.344439951
   RES
525268
1669772293.503582717
   RES
525120
1669772293.666182345
=================
Namespace(gnn='GCN', k=10, i=6, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.874, train acc: 0.005, val loss: 3.852, val acc: 0.002 (best val acc: 0.002))
In epoch 20, train loss: 1.007, train acc: 0.743, val loss: 1.148, val acc: 0.665 (best val acc: 0.665))
In epoch 40, train loss: 0.817, train acc: 0.777, val loss: 0.988, val acc: 0.720 (best val acc: 0.720))
In epoch 60, train loss: 0.740, train acc: 0.790, val loss: 0.916, val acc: 0.728 (best val acc: 0.729))
In epoch 80, train loss: 0.693, train acc: 0.798, val loss: 0.868, val acc: 0.732 (best val acc: 0.732))
   RES
525120
1669772293.826083335
   RES
525120
1669772293.983403683
   RES
491988
1669772294.143414619
   RES
475928
1669772294.301125525
   RES
475928
1669772294.461039050
   RES
459484
1669772294.619594244
   RES
459484
1669772294.778536316
   RES
475584
1669772294.937169153
   RES
451264
1669772295.095024870
   RES
451264
1669772295.253209540
   RES
483996
1669772295.412600714
   RES
467704
1669772295.570321946
   RES
467704
1669772295.728509033
   RES
467708
1669772295.887516248
=================
Namespace(gnn='GCN', k=10, i=4, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.741, train acc: 0.003, val loss: 3.746, val acc: 0.002 (best val acc: 0.002))
In epoch 20, train loss: 1.635, train acc: 0.651, val loss: 1.260, val acc: 0.739 (best val acc: 0.739))
In epoch 40, train loss: 1.250, train acc: 0.703, val loss: 0.965, val acc: 0.777 (best val acc: 0.777))
In epoch 60, train loss: 1.040, train acc: 0.737, val loss: 0.835, val acc: 0.792 (best val acc: 0.792))
In epoch 80, train loss: 0.926, train acc: 0.760, val loss: 0.783, val acc: 0.799 (best val acc: 0.799))
   RES
500368
1669772296.047446688
   RES
508680
1669772296.204228667
   RES
508680
1669772296.360156228
   RES
492368
1669772296.517692091
   RES
492368
1669772296.675155378
   RES
469968
1669772296.832683211
   RES
459484
1669772296.988798524
=================
Namespace(gnn='GCN', k=10, i=3, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.680, train acc: 0.013, val loss: 3.678, val acc: 0.008 (best val acc: 0.008))
In epoch 20, train loss: 2.287, train acc: 0.398, val loss: 2.175, val acc: 0.433 (best val acc: 0.439))
In epoch 40, train loss: 1.651, train acc: 0.542, val loss: 1.705, val acc: 0.509 (best val acc: 0.509))
In epoch 60, train loss: 1.431, train acc: 0.587, val loss: 1.541, val acc: 0.535 (best val acc: 0.536))
In epoch 80, train loss: 1.316, train acc: 0.614, val loss: 1.482, val acc: 0.549 (best val acc: 0.551))
   RES
459180
1669772297.146214520
   RES
459484
1669772297.303004128
   RES
459488
1669772297.458718754
   RES
459488
1669772297.614695364
   RES
475584
1669772297.770270960
   RES
483768
1669772297.926896522
   RES
467704
1669772298.082475384
   RES
475548
1669772298.237300092
   RES
492368
1669772298.392593456
   RES
492368
1669772298.547865335
   RES
516916
1669772298.703151255
=================
Namespace(gnn='GCN', k=10, i=8, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.615, train acc: 0.249, val loss: 3.689, val acc: 0.137 (best val acc: 0.137))
In epoch 20, train loss: 2.045, train acc: 0.464, val loss: 1.994, val acc: 0.445 (best val acc: 0.445))
In epoch 40, train loss: 1.642, train acc: 0.534, val loss: 1.557, val acc: 0.558 (best val acc: 0.558))
In epoch 60, train loss: 1.442, train acc: 0.571, val loss: 1.394, val acc: 0.572 (best val acc: 0.572))
In epoch 80, train loss: 1.305, train acc: 0.612, val loss: 1.308, val acc: 0.610 (best val acc: 0.611))
   RES
516916
1669772298.858072579
   RES
516916
1669772299.013669774
=================
Namespace(gnn='GCN', k=10, i=7, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.632, train acc: 0.030, val loss: 3.629, val acc: 0.024 (best val acc: 0.024))
In epoch 20, train loss: 1.614, train acc: 0.652, val loss: 1.897, val acc: 0.552 (best val acc: 0.568))
In epoch 40, train loss: 1.277, train acc: 0.699, val loss: 1.526, val acc: 0.599 (best val acc: 0.599))
In epoch 60, train loss: 1.116, train acc: 0.722, val loss: 1.361, val acc: 0.646 (best val acc: 0.646))
In epoch 80, train loss: 1.013, train acc: 0.734, val loss: 1.248, val acc: 0.677 (best val acc: 0.677))
   RES
459484
1669772299.169017593
   RES
467400
1669772299.325050172
   RES
467576
1669772299.479970120
   RES
484260
1669772299.634016848
   RES
484020
1669772299.788233530
   RES
467704
1669772299.942633168
=================
Namespace(gnn='GCN', k=10, i=5, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.698, train acc: 0.003, val loss: 3.699, val acc: 0.001 (best val acc: 0.001))
In epoch 20, train loss: 1.551, train acc: 0.634, val loss: 1.297, val acc: 0.663 (best val acc: 0.663))
In epoch 40, train loss: 1.280, train acc: 0.685, val loss: 1.141, val acc: 0.694 (best val acc: 0.694))
In epoch 60, train loss: 1.106, train acc: 0.716, val loss: 1.026, val acc: 0.719 (best val acc: 0.719))
In epoch 80, train loss: 1.007, train acc: 0.737, val loss: 0.944, val acc: 0.742 (best val acc: 0.742))
   RES
467708
1669772300.096312296
   RES
451268
1669772300.250609594
   RES
451008
1669772300.405226091
   RES
459488
1669772300.558817073
   RES
475328
1669772300.712785908
   RES
459480
1669772300.866561589
   RES
475924
1669772301.020680991
   RES
475664
1669772301.174213572
   RES
508808
1669772301.328119127
   RES
516464
1669772301.481845528
   RES
500588
1669772301.635456730
   RES
500328
1669772301.789416329
   RES
473968
1669772301.943804018
   RES
459480
1669772302.097046284
   RES
469480
1669772302.251608352
   RES
492240
1669772302.406367185
   RES
475944
1669772302.560295369
   RES
467448
1669772302.714295194
   RES
483892
1669772302.868577975
   RES
477936
1669772303.022768460
   RES
483548
1669772303.177158650
   RES
484152
1669772303.330891942
   RES
483892
1669772303.484348761
   RES
517152
1669772303.638155084
   RES
533008
1669772303.792189359
   RES
517032
1669772303.946471832
   RES
517028
1669772304.101136740
   RES
500024
1669772304.255056818
   RES
491720
1669772304.408318635
   RES
467396
1669772304.562009993
   RES
459480
1669772304.716484807
   RES
459488
1669772304.870600432
   RES
468688
1669772305.024912449
   RES
475852
1669772305.178507143
   RES
475804
1669772305.333020622
   RES
492480
1669772305.487470139
   RES
508584
1669772305.641257924
=================
Namespace(gnn='GCN', k=10, i=10, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.715, train acc: 0.016, val loss: 3.708, val acc: 0.018 (best val acc: 0.018))
In epoch 20, train loss: 2.211, train acc: 0.445, val loss: 2.292, val acc: 0.427 (best val acc: 0.427))
In epoch 40, train loss: 1.666, train acc: 0.565, val loss: 1.792, val acc: 0.538 (best val acc: 0.538))
In epoch 60, train loss: 1.444, train acc: 0.607, val loss: 1.574, val acc: 0.554 (best val acc: 0.554))
In epoch 80, train loss: 1.325, train acc: 0.631, val loss: 1.458, val acc: 0.575 (best val acc: 0.577))
   RES
508584
1669772305.795510509
   RES
466344
1669772305.949826189
   RES
459180
1669772306.103602393
   RES
500548
1669772306.256909284
