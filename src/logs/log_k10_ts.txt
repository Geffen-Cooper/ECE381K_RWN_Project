1669770619.327602321
=================
Namespace(k=10, dataset='arxiv')
training configuration:
Number of partitions: 10
Dataset: arxiv
LOAD ARXIV
cuda
DglNodePropPredDataset(1)
------
 (Graph(num_nodes=169343, num_edges=1166243,
      ndata_schemes={'year': Scheme(shape=(1,), dtype=torch.int64), 'feat': Scheme(shape=(128,), dtype=torch.float32)}
      edata_schemes={}), tensor([[ 4],
        [ 5],
        [28],
        ...,
        [10],
        [ 4],
        [ 1]]))
Number of categories: 40
partitioning  10  subgraphs
   RES
 83084
1669770672.026113053
   RES
132756
1669770672.180330096
   RES
198760
1669770672.334421892
   RES
213024
1669770672.488351020
   RES
232240
1669770672.642438070
   RES
255252
1669770672.796360109
   RES
271084
1669770672.951155648
   RES
277404
1669770673.106542764
   RES
281072
1669770673.262899174
   RES
283292
1669770673.420432309
   RES
283964
1669770673.576223161
   RES
295252
1669770673.733971001
   RES
309856
1669770673.891162075
   RES
324176
1669770674.052614028
   RES
330656
1669770674.215158890
   RES
330656
1669770674.376434409
   RES
340716
1669770674.535807696
   RES
412244
1669770674.694142615
   RES
425712
1669770674.858625421
   RES
426864
1669770675.020402053
   RES
437616
1669770675.182308860
   RES
503932
1669770675.344842166
   RES
479916
1669770675.505293918
   RES
495764
1669770675.666024015
   RES
512396
1669770675.830790897
   RES
512396
1669770675.996317192
   RES
520844
1669770676.163629416
   RES
520844
1669770676.331267845
   RES
521372
1669770676.496479995
   RES
521248
1669770676.661408533
   RES
520988
1669770676.824328502
   RES
520988
1669770676.990136834
   RES
521248
1669770677.154219269
   RES
521240
1669770677.315965642
   RES
504012
1669770677.476655768
   RES
504012
1669770677.642635422
   RES
495716
1669770677.816468754
   RES
520264
1669770677.985803263
   RES
503632
1669770678.150237300
   RES
512608
1669770678.309885201
   RES
512440
1669770678.471272195
   RES
512184
1669770678.631905920
   RES
494204
1669770678.794473088
   RES
502120
1669770678.956982320
   RES
494204
1669770679.120366945
   RES
527200
1669770679.284407390
   RES
535384
1669770679.448709979
   RES
510752
1669770679.612272448
   RES
510752
1669770679.773931802
   RES
510752
1669770679.936298030
   RES
510752
1669770680.099894595
   RES
510996
1669770680.262716192
   RES
510996
1669770680.429327242
   RES
510740
1669770680.592123921
   RES
519184
1669770680.752176861
   RES
519036
1669770680.914441074
   RES
518780
1669770681.075711409
   RES
518780
1669770681.236870886
   RES
519040
1669770681.402918324
   RES
519032
1669770681.563200567
   RES
518776
1669770681.724166792
   RES
518776
1669770681.890119842
   RES
518776
1669770682.049251909
   RES
526092
1669770682.207475534
   RES
526092
1669770682.366975699
   RES
509508
1669770682.524855545
   RES
509508
1669770682.684301995
   RES
509772
1669770682.847039747
   RES
509256
1669770683.007928157
   RES
525620
1669770683.168638096
   RES
526412
1669770683.332838282
   RES
526224
1669770683.493734519
   RES
533888
1669770683.653858824
   RES
534680
1669770683.815311305
   RES
534520
1669770683.976681053
   RES
534520
1669770684.134838422
   RES
534520
1669770684.295336564
   RES
534000
1669770684.456370542
   RES
534524
1669770684.617669237
   RES
534512
1669770684.779120605
   RES
534252
1669770684.944504513
   RES
534512
1669770685.104098777
   RES
534516
1669770685.262602637
   RES
534260
1669770685.421598143
   RES
500840
1669770685.587550665
   RES
501344
1669770685.747612333
   RES
501344
1669770685.906493581
   RES
501088
1669770686.064835404
   RES
525900
1669770686.228403775
   RES
526428
1669770686.392501230
   RES
526224
1669770686.558219758
   RES
526224
1669770686.724429158
   RES
526224
1669770686.884687298
   RES
526220
1669770687.042989368
   RES
526220
1669770687.204390326
   RES
526220
1669770687.364926045
   RES
526224
1669770687.522986083
   RES
526224
1669770687.684664353
   RES
525968
1669770687.850160801
   RES
498460
1669770688.009774314
   RES
498432
1669770688.167634542
   RES
498176
1669770688.326874553
   RES
498436
1669770688.489509424
   RES
498440
1669770688.648772219
   RES
498184
1669770688.810033089
   RES
497936
1669770688.972783214
   RES
498464
1669770689.131304673
   RES
498444
1669770689.291551246
   RES
498444
1669770689.453819325
   RES
498188
1669770689.615265562
   RES
498188
1669770689.774828330
   RES
498448
1669770689.936432148
   RES
498184
1669770690.095876120
   RES
498184
1669770690.255231894
   RES
498444
1669770690.413169914
   RES
498188
1669770690.572299080
   RES
496332
1669770690.731916045
   RES
463332
1669770690.891375334
   RES
463332
1669770691.051293943
   RES
463324
1669770691.211195627
   RES
495804
1669770691.370179880
   RES
503988
1669770691.531079991
   RES
504404
1669770691.692797461
   RES
504148
1669770691.853769140
   RES
471628
1669770692.012987775
   RES
471616
1669770692.170843375
   RES
487464
1669770692.328695194
   RES
495980
1669770692.489817887
   RES
496240
1669770692.649328327
   RES
496240
1669770692.808438112
   RES
495984
1669770692.968059941
   RES
528980
1669770693.126948314
   RES
529540
1669770693.286260327
   RES
529024
1669770693.446029992
   RES
529024
1669770693.605722994
   RES
529544
1669770693.762274490
   RES
487172
1669770693.921705964
   RES
487172
1669770694.080842044
   RES
484580
1669770694.238998732
   RES
484580
1669770694.399059371
   RES
485104
1669770694.557460969
   RES
485096
1669770694.715413216
   RES
497732
1669770694.876031579
   RES
506876
1669770695.037764609
   RES
506348
1669770695.195941001
   RES
514644
1669770695.354762347
   RES
514644
1669770695.513077433
   RES
514644
1669770695.675700447
   RES
514644
1669770695.838255127
   RES
514644
1669770695.998865550
   RES
514124
1669770696.155790721
   RES
514648
1669770696.312779982
   RES
514132
1669770696.471311022
   RES
495472
1669770696.631935018
   RES
495472
1669770696.791304407
   RES
503388
1669770696.954412634
   RES
486012
1669770697.115001195
   RES
507972
1669770697.271410115
   RES
511836
1669770697.430190468
   RES
512060
1669770697.590840776
   RES
528424
1669770697.751874442
   RES
528424
1669770697.914139299
   RES
528424
1669770698.074907090
   RES
529216
1669770698.235325588
   RES
529024
1669770698.396819709
   RES
511532
1669770698.556018727
   RES
512056
1669770698.718615515
   RES
512052
1669770698.879208806
   RES
511536
1669770699.038770399
   RES
502728
1669770699.203424860
   RES
503240
1669770699.364175275
   RES
502724
1669770699.523645144
   RES
502724
1669770699.681988835
   RES
502724
1669770699.842668557
   RES
503248
1669770700.002767074
   RES
503240
1669770700.163031052
   RES
502724
1669770700.330137320
   RES
503248
1669770700.527169479
   RES
502720
1669770700.707622563
   RES
503244
1669770700.867232016
   RES
502980
1669770701.028167454
   RES
502980
1669770701.191004950
   RES
502724
1669770701.349505075
   RES
491704
1669770701.509553523
   RES
492228
1669770701.674083042
   RES
491956
1669770701.835799537
   RES
491956
1669770701.995335593
   RES
500400
1669770702.158089214
   RES
500380
1669770702.318379917
   RES
500124
1669770702.479441772
   RES
500384
1669770702.642731785
   RES
500376
1669770702.802399505
   RES
500376
1669770702.961558260
   RES
500376
1669770703.119905224
   RES
500120
1669770703.279084602
   RES
500380
1669770703.437570349
   RES
500380
1669770703.596199101
   RES
500124
1669770703.756489295
   RES
498856
1669770703.919396158
   RES
499108
1669770704.080388851
   RES
506240
1669770704.238057574
   RES
515216
1669770704.398092765
   RES
515568
1669770704.567311102
   RES
515568
1669770704.727151532
   RES
515568
1669770704.887741885
   RES
498600
1669770705.049855780
   RES
498600
1669770705.216066214
   RES
498852
1669770705.380521959
   RES
498852
1669770705.541352182
   RES
498852
1669770705.704924246
   RES
499112
1669770705.868193940
   RES
499112
1669770706.029347466
   RES
498856
1669770706.190581929
   RES
498396
1669770706.352352938
   RES
498648
1669770706.517572926
   RES
498648
1669770706.680986820
   RES
498648
1669770706.841059598
   RES
498392
1669770707.002377317
   RES
506308
1669770707.163625227
   RES
490356
1669770707.327444651
   RES
490100
1669770707.488960412
   RES
490100
1669770707.649311260
   RES
490100
1669770707.810510766
   RES
490084
1669770707.971471294
   RES
490084
1669770708.134035077
   RES
514520
1669770708.295727356
   RES
498572
1669770708.455834403
   RES
498564
1669770708.615507762
   RES
493456
1669770708.776214479
   RES
497416
1669770708.937455027
   RES
497944
1669770709.098036400
   RES
497944
1669770709.258245086
   RES
497924
1669770709.418702032
   RES
497668
1669770709.576430182
   RES
506088
1669770709.741918401
   RES
487052
1669770709.906513455
   RES
479396
1669770710.071724198
   RES
479392
1669770710.233318851
   RES
479392
1669770710.399686692
   RES
479136
1669770710.566839683
   RES
484092
1669770710.730034931
   RES
484092
1669770710.892847592
   RES
484352
1669770711.059222516
   RES
484348
1669770711.218054734
   RES
484092
1669770711.381607885
   RES
484092
1669770711.546591908
   RES
484092
1669770711.712740326
   RES
484092
1669770711.872771695
   RES
484344
1669770712.033853869
   RES
492004
1669770712.198001473
   RES
517084
1669770712.370297260
   RES
517612
1669770712.538473168
   RES
517392
1669770712.700677235
   RES
517136
1669770712.861162089
   RES
508592
1669770713.022658188
   RES
509120
1669770713.186037339
   RES
509092
1669770713.348711760
   RES
508836
1669770713.514894684
   RES
517256
1669770713.678032894
   RES
517256
1669770713.841435591
   RES
517516
1669770714.000551629
   RES
517516
1669770714.159891700
   RES
517260
1669770714.321592653
   RES
525176
1669770714.484601223
   RES
525680
1669770714.644108065
   RES
525168
1669770714.806363999
   RES
508708
1669770714.971500729
   RES
517416
1669770715.133512991
   RES
517392
1669770715.291792811
   RES
491612
1669770715.452248824
   RES
499572
1669770715.615570395
   RES
507820
1669770715.778381869
   RES
507820
1669770715.937551393
   RES
508612
1669770716.098913320
   RES
508580
1669770716.263313123
   RES
508064
1669770716.429787218
   RES
508064
1669770716.591739474
   RES
508064
1669770716.753217589
   RES
508064
1669770716.917676265
   RES
508588
1669770717.079027833
   RES
508584
1669770717.242309233
   RES
508068
1669770717.403783365
   RES
508068
1669770717.568152727
   RES
508592
1669770717.730815288
   RES
508580
1669770717.888502987
   RES
508064
1669770718.050227440
   RES
508064
1669770718.216004284
   RES
508064
1669770718.378121387
   RES
508580
1669770718.537582412
   RES
508584
1669770718.700535253
   RES
508068
1669770718.861477703
   RES
515720
1669770719.022025511
   RES
516292
1669770719.183145569
   RES
515776
1669770719.344549977
   RES
509084
1669770719.505930311
   RES
499840
1669770719.665984958
   RES
499576
1669770719.825029969
   RES
499576
1669770719.984270555
   RES
499320
1669770720.144545455
   RES
507236
1669770720.303968583
   RES
508028
1669770720.465175336
   RES
508000
1669770720.625470901
   RES
491252
1669770720.784159088
   RES
491512
1669770720.942092884
   RES
491512
1669770721.100003045
   RES
491256
1669770721.262981470
   RES
489672
1669770721.427591761
   RES
497964
1669770721.587683839
   RES
479136
1669770721.748783077
   RES
484676
1669770721.908433845
   RES
508964
1669770722.081275282
   RES
476288
1669770722.244251306
   RES
476288
1669770722.404539031
   RES
476288
1669770722.563783751
   RES
476032
1669770722.723975380
   RES
492616
1669770722.885682529
   RES
492868
1669770723.051470380
   RES
473436
1669770723.214250767
   RES
490020
1669770723.374834041
   RES
473348
1669770723.534587456
   RES
473348
1669770723.695283788
   RES
481008
1669770723.853251846
   RES
481384
1669770724.015878904
   RES
497968
1669770724.176217983
   RES
487884
1669770724.337646886
   RES
511932
1669770724.501148438
   RES
498220
1669770724.665425099
   RES
497752
1669770724.826448725
   RES
497752
1669770724.987412105
   RES
498012
1669770725.151479734
   RES
497492
1669770725.311289511
   RES
539064
1669770725.471551390
   RES
547492
1669770725.631089246
   RES
547800
1669770725.793085626
   RES
548056
1669770725.959429986
   RES
531344
1669770726.125633927
   RES
555892
1669770726.285459313
   RES
555892
1669770726.447872116
   RES
555892
1669770726.607788623
   RES
547032
1669770726.766073956
   RES
547032
1669770726.926949263
   RES
547032
1669770727.090984725
   RES
547032
1669770727.249871660
   RES
523048
1669770727.410493127
   RES
523048
1669770727.576101169
   RES
523048
1669770727.736322332
   RES
556220
1669770727.897013214
   RES
539260
1669770728.056870045
   RES
547708
1669770728.214818572
   RES
547708
1669770728.375892706
   RES
547708
1669770728.537771580
   RES
547708
1669770728.701390039
   RES
547708
1669770728.861220523
   RES
514756
1669770729.023238801
   RES
531648
1669770729.184184725
   RES
547752
1669770729.342428330
   RES
531344
1669770729.504438399
   RES
531344
1669770729.666268971
   RES
539524
1669770729.823913563
   RES
539280
1669770729.982335099
   RES
539024
1669770730.143506304
   RES
539024
1669770730.306040721
   RES
537908
1669770730.468647486
   RES
538436
1669770730.630956460
   RES
538420
1669770730.789541434
   RES
538420
1669770730.949354718
   RES
538128
1669770731.109042758
   RES
538128
1669770731.269283101
   RES
544460
1669770731.428563656
   RES
525644
1669770731.588617278
   RES
522536
1669770731.748615888
   RES
522160
1669770731.910263572
   RES
522160
1669770732.070750861
   RES
522536
1669770732.231423256
   RES
522536
1669770732.393143761
   RES
522536
1669770732.552972979
   RES
506468
1669770732.711134306
   RES
514384
1669770732.871385580
   RES
522676
1669770733.034114390
   RES
522676
1669770733.196892164
   RES
530596
1669770733.356680162
   RES
514244
1669770733.514455156
   RES
514244
1669770733.673892987
   RES
514244
1669770733.834578389
   RES
514244
1669770733.998469437
   RES
508060
1669770734.160069225
   RES
514396
1669770734.318245069
   RES
514392
1669770734.476558490
   RES
514136
1669770734.636456024
   RES
514136
1669770734.798499854
   RES
506232
1669770734.960023282
   RES
506236
1669770735.122486041
   RES
505980
1669770735.286943149
   RES
505980
1669770735.450406292
   RES
506120
1669770735.609989262
   RES
497828
1669770735.770243220
=================
Namespace(gnn='GCN', k=10, i=4, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.933, train acc: 0.012, val loss: 3.949, val acc: 0.006 (best val acc: 0.006))
In epoch 20, train loss: 1.689, train acc: 0.633, val loss: 1.318, val acc: 0.737 (best val acc: 0.737))
In epoch 40, train loss: 1.302, train acc: 0.691, val loss: 1.002, val acc: 0.770 (best val acc: 0.770))
In epoch 60, train loss: 1.078, train acc: 0.725, val loss: 0.859, val acc: 0.789 (best val acc: 0.789))
In epoch 80, train loss: 0.954, train acc: 0.752, val loss: 0.801, val acc: 0.796 (best val acc: 0.796))
In epoch 0, student train loss: 3.373, student train acc: 0.020, student val loss: 3.370, student val acc: 0.025 (best student val acc: 0.025))
In epoch 20, student train loss: 1.734, student train acc: 0.633, student val loss: 1.218, student val acc: 0.737 (best student val acc: 0.737))
In epoch 40, student train loss: 1.597, student train acc: 0.633, student val loss: 1.191, student val acc: 0.737 (best student val acc: 0.737))
In epoch 60, student train loss: 1.499, student train acc: 0.633, student val loss: 1.127, student val acc: 0.737 (best student val acc: 0.737))
In epoch 80, student train loss: 1.401, student train acc: 0.651, student val loss: 1.049, student val acc: 0.738 (best student val acc: 0.738))
   RES
498204
1669770735.931943559
   RES
498204
1669770736.091204073
   RES
528824
1669770736.250643894
   RES
514412
1669770736.409273948
   RES
514412
1669770736.568481775
   RES
498204
1669770736.729075904
   RES
498204
1669770736.890085374
   RES
523080
1669770737.050036777
   RES
497828
1669770737.208842378
   RES
509972
1669770737.367586780
   RES
497828
1669770737.525207512
   RES
511028
1669770737.682269523
   RES
497828
1669770737.838615717
   RES
497828
1669770737.996822097
   RES
497828
1669770738.155349921
   RES
497828
1669770738.313019998
   RES
506120
1669770738.470021253
   RES
497828
1669770738.627358084
   RES
506120
1669770738.785371851
=================
Namespace(gnn='GCN', k=10, i=6, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.750, train acc: 0.001, val loss: 3.728, val acc: 0.000 (best val acc: 0.000))
In epoch 20, train loss: 0.990, train acc: 0.732, val loss: 1.143, val acc: 0.645 (best val acc: 0.679))
In epoch 40, train loss: 0.824, train acc: 0.777, val loss: 0.983, val acc: 0.723 (best val acc: 0.723))
In epoch 60, train loss: 0.739, train acc: 0.791, val loss: 0.909, val acc: 0.729 (best val acc: 0.730))
In epoch 80, train loss: 0.681, train acc: 0.800, val loss: 0.851, val acc: 0.737 (best val acc: 0.737))
In epoch 0, student train loss: 3.317, student train acc: 0.004, student val loss: 3.323, student val acc: 0.005 (best student val acc: 0.005))
In epoch 20, student train loss: 1.084, student train acc: 0.719, student val loss: 1.245, student val acc: 0.622 (best student val acc: 0.622))
In epoch 40, student train loss: 0.934, student train acc: 0.719, student val loss: 1.082, student val acc: 0.622 (best student val acc: 0.622))
In epoch 60, student train loss: 0.852, student train acc: 0.738, student val loss: 0.995, student val acc: 0.657 (best student val acc: 0.657))
In epoch 80, student train loss: 0.790, student train acc: 0.770, student val loss: 0.932, student val acc: 0.718 (best student val acc: 0.718))
   RES
497828
1669770738.944678900
   RES
546668
1669770739.104131632
   RES
514412
1669770739.261690273
   RES
506496
1669770739.421503709
   RES
497828
1669770739.580075143
=================
Namespace(gnn='GCN', k=10, i=8, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.764, train acc: 0.007, val loss: 3.778, val acc: 0.003 (best val acc: 0.003))
In epoch 20, train loss: 2.103, train acc: 0.455, val loss: 2.071, val acc: 0.412 (best val acc: 0.425))
In epoch 40, train loss: 1.693, train acc: 0.519, val loss: 1.587, val acc: 0.524 (best val acc: 0.540))
In epoch 60, train loss: 1.470, train acc: 0.565, val loss: 1.411, val acc: 0.553 (best val acc: 0.558))
In epoch 80, train loss: 1.330, train acc: 0.604, val loss: 1.320, val acc: 0.583 (best val acc: 0.583))
In epoch 0, student train loss: 3.365, student train acc: 0.023, student val loss: 3.356, student val acc: 0.025 (best student val acc: 0.025))
In epoch 20, student train loss: 2.308, student train acc: 0.274, student val loss: 2.293, student val acc: 0.140 (best student val acc: 0.140))
In epoch 40, student train loss: 2.170, student train acc: 0.280, student val loss: 2.202, student val acc: 0.155 (best student val acc: 0.155))
In epoch 60, student train loss: 1.989, student train acc: 0.430, student val loss: 1.962, student val acc: 0.405 (best student val acc: 0.409))
In epoch 80, student train loss: 1.824, student train acc: 0.455, student val loss: 1.757, student val acc: 0.411 (best student val acc: 0.411))
   RES
534260
1669770739.736610080
   RES
498204
1669770739.894151756
   RES
547568
1669770740.049466391
   RES
547832
1669770740.205386226
   RES
548096
1669770740.362098808
   RES
548064
1669770740.517118617
   RES
572620
1669770740.672700526
   RES
581332
1669770740.828365061
   RES
581232
1669770740.984417346
   RES
580976
1669770741.140678080
   RES
555968
1669770741.296515071
   RES
555964
1669770741.453109954
   RES
555708
1669770741.608716911
   RES
489888
1669770741.764085438
   RES
489888
1669770741.919750461
   RES
489884
1669770742.074602300
   RES
489628
1669770742.230150325
   RES
530808
1669770742.386674398
=================
Namespace(gnn='GCN', k=10, i=7, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.678, train acc: 0.008, val loss: 3.663, val acc: 0.004 (best val acc: 0.004))
In epoch 20, train loss: 1.619, train acc: 0.650, val loss: 1.899, val acc: 0.542 (best val acc: 0.544))
In epoch 40, train loss: 1.292, train acc: 0.695, val loss: 1.566, val acc: 0.591 (best val acc: 0.591))
In epoch 60, train loss: 1.124, train acc: 0.718, val loss: 1.375, val acc: 0.640 (best val acc: 0.640))
In epoch 80, train loss: 1.023, train acc: 0.734, val loss: 1.262, val acc: 0.672 (best val acc: 0.672))
In epoch 0, student train loss: 3.315, student train acc: 0.036, student val loss: 3.313, student val acc: 0.046 (best student val acc: 0.046))
In epoch 20, student train loss: 1.932, student train acc: 0.495, student val loss: 2.179, student val acc: 0.321 (best student val acc: 0.325))
In epoch 40, student train loss: 1.635, student train acc: 0.558, student val loss: 1.907, student val acc: 0.460 (best student val acc: 0.460))
In epoch 60, student train loss: 1.471, student train acc: 0.615, student val loss: 1.729, student val acc: 0.528 (best student val acc: 0.528))
In epoch 80, student train loss: 1.358, student train acc: 0.638, student val loss: 1.613, student val acc: 0.531 (best student val acc: 0.535))
   RES
498336
1669770742.542383037
   RES
497792
1669770742.697631330
=================
Namespace(gnn='GCN', k=10, i=5, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.693, train acc: 0.077, val loss: 3.678, val acc: 0.139 (best val acc: 0.139))
In epoch 20, train loss: 1.553, train acc: 0.635, val loss: 1.329, val acc: 0.664 (best val acc: 0.673))
In epoch 40, train loss: 1.310, train acc: 0.678, val loss: 1.160, val acc: 0.695 (best val acc: 0.695))
In epoch 60, train loss: 1.138, train acc: 0.713, val loss: 1.044, val acc: 0.718 (best val acc: 0.718))
In epoch 80, train loss: 1.028, train acc: 0.737, val loss: 0.956, val acc: 0.743 (best val acc: 0.744))
In epoch 0, student train loss: 3.316, student train acc: 0.012, student val loss: 3.313, student val acc: 0.006 (best student val acc: 0.006))
In epoch 20, student train loss: 1.592, student train acc: 0.630, student val loss: 1.292, student val acc: 0.661 (best student val acc: 0.661))
In epoch 40, student train loss: 1.461, student train acc: 0.630, student val loss: 1.240, student val acc: 0.661 (best student val acc: 0.661))
In epoch 60, student train loss: 1.358, student train acc: 0.631, student val loss: 1.175, student val acc: 0.661 (best student val acc: 0.661))
In epoch 80, student train loss: 1.215, student train acc: 0.664, student val loss: 1.067, student val acc: 0.681 (best student val acc: 0.681))
   RES
506212
1669770742.852842200
=================
Namespace(gnn='GCN', k=10, i=3, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.689, train acc: 0.011, val loss: 3.671, val acc: 0.005 (best val acc: 0.005))
In epoch 20, train loss: 2.273, train acc: 0.416, val loss: 2.182, val acc: 0.435 (best val acc: 0.435))
In epoch 40, train loss: 1.640, train acc: 0.544, val loss: 1.695, val acc: 0.508 (best val acc: 0.508))
In epoch 60, train loss: 1.424, train acc: 0.587, val loss: 1.545, val acc: 0.535 (best val acc: 0.535))
In epoch 80, train loss: 1.314, train acc: 0.614, val loss: 1.485, val acc: 0.553 (best val acc: 0.555))
In epoch 0, student train loss: 3.313, student train acc: 0.001, student val loss: 3.300, student val acc: 0.000 (best student val acc: 0.000))
In epoch 20, student train loss: 2.690, student train acc: 0.206, student val loss: 2.451, student val acc: 0.338 (best student val acc: 0.338))
In epoch 40, student train loss: 2.442, student train acc: 0.261, student val loss: 2.241, student val acc: 0.375 (best student val acc: 0.375))
In epoch 60, student train loss: 2.081, student train acc: 0.373, student val loss: 1.967, student val acc: 0.419 (best student val acc: 0.419))
In epoch 80, student train loss: 1.798, student train acc: 0.454, student val loss: 1.745, student val acc: 0.465 (best student val acc: 0.465))
   RES
506728
1669770743.007757892
   RES
497544
1669770743.162720057
   RES
497544
1669770743.317643395
   RES
497544
1669770743.471916481
   RES
497544
1669770743.626735775
   RES
506212
1669770743.780842298
   RES
506736
1669770743.934823134
   RES
528392
1669770744.088525470
   RES
523056
1669770744.242915914
   RES
530716
1669770744.397238604
   RES
539956
1669770744.551383849
   RES
539776
1669770744.705334150
   RES
505696
1669770744.859449626
   RES
506212
1669770745.013782559
   RES
523852
1669770745.167326735
   RES
522664
1669770745.321487080
   RES
547696
1669770745.475622126
   RES
548060
1669770745.629736387
   RES
531348
1669770745.783651331
   RES
556424
1669770745.937822781
   RES
556352
1669770746.091689431
   RES
539896
1669770746.245707008
   RES
539896
1669770746.399558871
   RES
506612
1669770746.553508159
   RES
506596
1669770746.706761152
   RES
498056
1669770746.860468784
   RES
506612
1669770747.014056978
   RES
533432
1669770747.167483970
   RES
523180
1669770747.321285985
   RES
556176
1669770747.475441287
   RES
556440
1669770747.629134824
   RES
556352
1669770747.783176918
   RES
523196
1669770747.937317843
   RES
523180
1669770748.091233344
   RES
506724
1669770748.244802097
   RES
490140
1669770748.398779246
   RES
498432
1669770748.553099336
   RES
498432
1669770748.707078165
   RES
539792
1669770748.860491026
   RES
530904
1669770749.013906354
   RES
539944
1669770749.167671451
   RES
539512
1669770749.321728550
   RES
548192
1669770749.475877055
   RES
548188
1669770749.629727503
   RES
548192
1669770749.783952658
   RES
547932
1669770749.937896177
   RES
548192
1669770750.092047018
   RES
547932
1669770750.245698435
   RES
539272
1669770750.399149949
   RES
538992
1669770750.552871294
   RES
506620
1669770750.706913782
   RES
497800
1669770750.861385835
   RES
506620
1669770751.015379225
   RES
514432
1669770751.169771870
   RES
523204
1669770751.323526070
   RES
522924
1669770751.477245511
   RES
531632
1669770751.630843950
   RES
531220
1669770751.784713004
   RES
498436
1669770751.939042510
   RES
498176
1669770752.093576494
   RES
498328
1669770752.247977937
   RES
498052
1669770752.401762068
   RES
514916
1669770752.555832642
   RES
489888
1669770752.709634362
   RES
539904
1669770752.864301266
   RES
523052
1669770753.018325122
   RES
514504
1669770753.172557410
   RES
514248
1669770753.326812138
   RES
511916
1669770753.481216521
   RES
511656
1669770753.635259589
   RES
506596
1669770753.789354714
   RES
506340
1669770753.943403875
   RES
514912
1669770754.096912338
=================
Namespace(gnn='GCN', k=10, i=2, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.671, train acc: 0.006, val loss: 3.666, val acc: 0.001 (best val acc: 0.001))
In epoch 20, train loss: 1.761, train acc: 0.572, val loss: 1.565, val acc: 0.634 (best val acc: 0.634))
In epoch 40, train loss: 1.466, train acc: 0.597, val loss: 1.364, val acc: 0.644 (best val acc: 0.644))
In epoch 60, train loss: 1.239, train acc: 0.667, val loss: 1.177, val acc: 0.686 (best val acc: 0.686))
In epoch 80, train loss: 1.127, train acc: 0.687, val loss: 1.089, val acc: 0.701 (best val acc: 0.701))
In epoch 0, student train loss: 3.417, student train acc: 0.006, student val loss: 3.416, student val acc: 0.002 (best student val acc: 0.002))
In epoch 20, student train loss: 1.794, student train acc: 0.572, student val loss: 1.444, student val acc: 0.634 (best student val acc: 0.634))
In epoch 40, student train loss: 1.642, student train acc: 0.572, student val loss: 1.433, student val acc: 0.634 (best student val acc: 0.634))
In epoch 60, student train loss: 1.550, student train acc: 0.572, student val loss: 1.371, student val acc: 0.634 (best student val acc: 0.634))
In epoch 80, student train loss: 1.437, student train acc: 0.584, student val loss: 1.277, student val acc: 0.637 (best student val acc: 0.637))
   RES
514888
1669770754.250647686
   RES
539964
1669770754.404599301
   RES
529960
1669770754.558131157
   RES
506732
1669770754.712013745
   RES
515024
1669770754.866231187
   RES
521932
1669770755.020364441
   RES
509328
1669770755.173634620
   RES
509084
1669770755.327252560
   RES
509072
1669770755.481580648
   RES
517492
1669770755.635834941
   RES
539376
1669770755.789560808
   RES
531244
1669770755.943103643
   RES
523328
1669770756.096459990
   RES
531500
1669770756.250227614
   RES
556564
1669770756.403980475
   RES
513644
1669770756.557545412
   RES
498068
1669770756.711394307
   RES
498444
1669770756.864718654
   RES
498316
1669770757.018426446
   RES
490152
1669770757.172530895
   RES
515028
1669770757.326214800
   RES
498316
1669770757.480557765
   RES
514652
1669770757.635079770
   RES
514652
1669770757.789493853
   RES
539792
1669770757.943303856
