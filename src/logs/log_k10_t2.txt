1669772540.418991840
=================
Namespace(k=10, dataset='arxiv')
training configuration:
Number of partitions: 10
Dataset: arxiv
LOAD ARXIV
cuda
DglNodePropPredDataset(1)
------
 (Graph(num_nodes=169343, num_edges=1166243,
      ndata_schemes={'year': Scheme(shape=(1,), dtype=torch.int64), 'feat': Scheme(shape=(128,), dtype=torch.float32)}
      edata_schemes={}), tensor([[ 4],
        [ 5],
        [28],
        ...,
        [10],
        [ 4],
        [ 1]]))
Number of categories: 40
partitioning  10  subgraphs
   RES
114832
1669772594.771169804
   RES
200232
1669772594.925783314
   RES
232344
1669772595.080389244
   RES
259048
1669772595.242607497
   RES
280244
1669772595.396893338
   RES
285760
1669772595.550828167
   RES
299412
1669772595.704976326
   RES
333160
1669772595.859180911
   RES
401544
1669772596.015006150
   RES
431064
1669772596.171000520
   RES
490916
1669772596.328261470
   RES
499928
1669772596.486939972
   RES
516108
1669772596.644487393
   RES
532208
1669772596.805662335
   RES
507880
1669772596.967333146
   RES
482840
1669772597.126643698
   RES
483100
1669772597.284361569
   RES
483096
1669772597.442596333
   RES
522704
1669772597.605934332
   RES
498452
1669772597.769285352
   RES
503732
1669772597.931831595
   RES
506864
1669772598.114913387
   RES
515048
1669772598.320764020
   RES
515048
1669772598.503477271
   RES
515048
1669772598.668969156
   RES
515576
1669772598.834361847
   RES
515368
1669772598.998238279
   RES
476876
1669772599.161747829
   RES
474760
1669772599.324479372
   RES
482348
1669772599.489770618
   RES
515456
1669772599.653095329
   RES
515456
1669772599.817870808
   RES
482420
1669772599.981491589
   RES
507236
1669772600.144716821
   RES
507236
1669772600.311074632
   RES
507244
1669772600.473820835
   RES
523608
1669772600.635659820
   RES
523940
1669772600.803421905
   RES
523684
1669772600.970180632
   RES
523940
1669772601.132915998
   RES
523684
1669772601.294665864
   RES
523684
1669772601.457508432
   RES
523944
1669772601.622712342
   RES
523684
1669772601.785271603
   RES
523684
1669772601.947005728
   RES
523684
1669772602.110370135
   RES
507068
1669772602.272621333
   RES
507628
1669772602.431917204
   RES
502736
1669772602.591227467
   RES
474756
1669772602.751064770
   RES
474756
1669772602.909729297
   RES
515412
1669772603.069937379
   RES
523944
1669772603.231285131
   RES
523944
1669772603.393595924
   RES
523944
1669772603.556618811
   RES
523944
1669772603.716972446
   RES
507592
1669772603.876740174
   RES
507508
1669772604.035406729
   RES
507508
1669772604.194333740
   RES
523872
1669772604.360151444
   RES
524136
1669772604.520550732
   RES
523948
1669772604.679536066
   RES
523948
1669772604.841328444
   RES
523948
1669772605.004994976
   RES
523948
1669772605.169229926
   RES
523948
1669772605.331140584
   RES
523948
1669772605.491792665
   RES
523948
1669772605.654254310
   RES
523948
1669772605.815026074
   RES
515816
1669772605.974987295
   RES
515728
1669772606.135214035
   RES
506820
1669772606.301348674
   RES
507000
1669772606.464451148
   RES
507000
1669772606.623013336
   RES
506996
1669772606.778979664
   RES
506996
1669772606.939220665
   RES
506996
1669772607.102919799
   RES
506996
1669772607.263009226
   RES
506996
1669772607.424157725
   RES
506996
1669772607.583984359
   RES
506996
1669772607.744186007
   RES
506740
1669772607.904873961
   RES
506996
1669772608.063480511
   RES
506740
1669772608.226643714
   RES
506996
1669772608.396954945
   RES
506740
1669772608.557561319
   RES
488332
1669772608.717374850
   RES
488596
1669772608.877811225
   RES
488244
1669772609.035949078
   RES
486808
1669772609.196379407
   RES
487596
1669772609.366515668
   RES
519504
1669772609.533714969
   RES
528216
1669772609.693361243
   RES
527784
1669772609.852493875
   RES
519916
1669772610.015378398
   RES
519564
1669772610.176819552
   RES
503252
1669772610.339018802
   RES
503512
1669772610.498933607
   RES
503252
1669772610.658377686
   RES
503512
1669772610.819665564
   RES
503252
1669772610.977252965
   RES
486064
1669772611.135582818
   RES
493724
1669772611.293315922
   RES
501908
1669772611.452199696
   RES
502368
1669772611.610581509
   RES
465028
1669772611.769315894
   RES
479848
1669772611.928615025
   RES
487468
1669772612.087533215
   RES
458316
1669772612.245089945
   RES
501088
1669772612.404734080
   RES
491200
1669772612.564093511
   RES
507044
1669772612.721613964
   RES
515228
1669772612.882295861
   RES
515728
1669772613.042236783
   RES
515476
1669772613.202557763
   RES
498860
1669772613.363055749
   RES
491200
1669772613.521822656
   RES
490944
1669772613.681964376
   RES
490944
1669772613.842345961
   RES
474504
1669772614.001032765
   RES
463976
1669772614.160328536
   RES
463976
1669772614.318947598
   RES
463716
1669772614.478445139
   RES
471632
1669772614.640787392
   RES
463856
1669772614.805058557
   RES
463828
1669772614.964802180
   RES
466724
1669772615.125440999
   RES
488484
1669772615.289330050
   RES
472024
1669772615.447473548
   RES
455460
1669772615.607778656
   RES
496640
1669772615.770528913
   RES
496432
1669772615.931047224
   RES
496428
1669772616.090257932
   RES
480116
1669772616.249991567
   RES
480116
1669772616.409014517
   RES
479600
1669772616.568590320
   RES
474464
1669772616.727952330
   RES
474464
1669772616.886411252
   RES
474464
1669772617.047111511
   RES
474464
1669772617.209145688
   RES
476572
1669772617.372541034
   RES
488280
1669772617.534439069
   RES
468964
1669772617.695065844
   RES
477056
1669772617.860601845
   RES
476752
1669772618.021093985
   RES
476752
1669772618.180193746
   RES
492368
1669772618.339452657
   RES
492368
1669772618.497896396
   RES
491856
1669772618.657000531
   RES
508484
1669772618.817114521
   RES
508384
1669772618.974991399
   RES
508384
1669772619.133564115
   RES
490648
1669772619.293387190
   RES
523380
1669772619.454516874
   RES
490340
1669772619.614300770
   RES
506708
1669772619.778372919
   RES
506972
1669772619.938992560
   RES
471676
1669772620.099431052
   RES
471676
1669772620.259112698
   RES
479080
1669772620.417306620
   RES
479644
1669772620.576080058
   RES
479600
1669772620.736003878
   RES
471636
1669772620.895315649
   RES
471636
1669772621.055004048
   RES
477328
1669772621.214322922
   RES
477328
1669772621.373806027
   RES
476816
1669772621.531539695
   RES
501476
1669772621.691834231
   RES
517576
1669772621.852082035
   RES
501172
1669772622.010471518
   RES
482468
1669772622.169528617
   RES
499096
1669772622.331592412
   RES
498996
1669772622.491627664
   RES
482684
1669772622.653772432
   RES
490600
1669772622.811772414
   RES
523600
1669772622.972508615
   RES
523864
1669772623.133547396
   RES
523656
1669772623.296465984
   RES
480112
1669772623.456295166
   RES
488024
1669772623.614972981
   RES
521024
1669772623.773566331
   RES
529304
1669772623.932106172
   RES
504772
1669772624.090055987
=================
Namespace(gnn='GCN', k=10, i=6, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.731, train acc: 0.000, val loss: 3.734, val acc: 0.000 (best val acc: 0.000))
In epoch 20, train loss: 0.973, train acc: 0.728, val loss: 1.128, val acc: 0.638 (best val acc: 0.657))
In epoch 40, train loss: 0.815, train acc: 0.787, val loss: 0.984, val acc: 0.721 (best val acc: 0.723))
In epoch 60, train loss: 0.738, train acc: 0.793, val loss: 0.903, val acc: 0.731 (best val acc: 0.732))
In epoch 80, train loss: 0.682, train acc: 0.803, val loss: 0.855, val acc: 0.738 (best val acc: 0.740))
   RES
512688
1669772624.250359476
   RES
512952
1669772624.409099362
   RES
512864
1669772624.571107475
   RES
512864
1669772624.729909178
=================
Namespace(gnn='GCN', k=10, i=8, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.681, train acc: 0.057, val loss: 3.643, val acc: 0.134 (best val acc: 0.134))
In epoch 20, train loss: 2.036, train acc: 0.460, val loss: 1.940, val acc: 0.429 (best val acc: 0.432))
In epoch 40, train loss: 1.647, train acc: 0.530, val loss: 1.541, val acc: 0.560 (best val acc: 0.562))
In epoch 60, train loss: 1.445, train acc: 0.572, val loss: 1.391, val acc: 0.563 (best val acc: 0.567))
In epoch 80, train loss: 1.322, train acc: 0.604, val loss: 1.317, val acc: 0.587 (best val acc: 0.588))
   RES
512864
1669772624.888266724
   RES
499380
1669772625.045960509
   RES
507340
1669772625.203610582
=================
Namespace(gnn='GCN', k=10, i=3, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.760, train acc: 0.013, val loss: 3.747, val acc: 0.008 (best val acc: 0.008))
In epoch 20, train loss: 2.355, train acc: 0.401, val loss: 2.242, val acc: 0.435 (best val acc: 0.435))
In epoch 40, train loss: 1.691, train acc: 0.534, val loss: 1.739, val acc: 0.509 (best val acc: 0.509))
In epoch 60, train loss: 1.461, train acc: 0.582, val loss: 1.568, val acc: 0.529 (best val acc: 0.529))
In epoch 80, train loss: 1.344, train acc: 0.609, val loss: 1.498, val acc: 0.544 (best val acc: 0.544))
   RES
447012
1669772625.367396035
=================
Namespace(gnn='GCN', k=10, i=4, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.792, train acc: 0.005, val loss: 3.785, val acc: 0.004 (best val acc: 0.004))
In epoch 20, train loss: 1.641, train acc: 0.633, val loss: 1.257, val acc: 0.737 (best val acc: 0.737))
In epoch 40, train loss: 1.232, train acc: 0.703, val loss: 0.953, val acc: 0.780 (best val acc: 0.780))
In epoch 60, train loss: 1.028, train acc: 0.740, val loss: 0.833, val acc: 0.792 (best val acc: 0.792))
In epoch 80, train loss: 0.920, train acc: 0.762, val loss: 0.783, val acc: 0.799 (best val acc: 0.799))
   RES
447012
1669772625.526113065
=================
Namespace(gnn='GCN', k=10, i=5, dataset='arxiv', heads=3, dropout=0.25, no_cuda=False, student_only=False, compression_rate='big')
training configuration:
Model:  GCN
Number of partitions: 10
Dataset: arxiv
num_heads:  3
cpu
# teacher params 21672
# student params 2068
In epoch 0, train loss: 3.808, train acc: 0.000, val loss: 3.801, val acc: 0.000 (best val acc: 0.000))
In epoch 20, train loss: 1.591, train acc: 0.633, val loss: 1.387, val acc: 0.662 (best val acc: 0.677))
In epoch 40, train loss: 1.366, train acc: 0.650, val loss: 1.197, val acc: 0.672 (best val acc: 0.677))
In epoch 60, train loss: 1.195, train acc: 0.699, val loss: 1.084, val acc: 0.703 (best val acc: 0.703))
In epoch 80, train loss: 1.078, train acc: 0.723, val loss: 1.007, val acc: 0.722 (best val acc: 0.722))
   RES
446500
1669772625.683145241
   RES
446500
1669772625.839199418